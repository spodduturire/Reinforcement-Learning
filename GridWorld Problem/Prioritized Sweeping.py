# -*- coding: utf-8 -*-
"""RLPSGridWorldFinalVersion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XVw1pZslzeoRRzzJbbJXgeRfbhYhh_n5
"""

import numpy as np
import matplotlib.pyplot as plt

def go_up(current_state):
  return (current_state[0]-1, current_state[1])

def go_down(current_state):
  return (current_state[0]+1, current_state[1])

def go_left(current_state):
  return (current_state[0], current_state[1]-1)

def go_right(current_state):
  return (current_state[0], current_state[1]+1)

def get_reward(next_state, gold):
  if next_state == (0, 2):
    return gold
  if next_state == (4, 2):
    return -10
  if next_state == (4, 4):
    return 10
  return 0

def is_valid_state(current_state):
  return current_state[0] > -1 and current_state[0] < 5 and current_state[1] > -1 and current_state[1] < 5 and current_state != (2, 2) and current_state != (3, 2)

def add_to_dic(dic, state, current_state, p):
  if is_valid_state(state):
    if state in dic.keys():
      dic[state] += p
    else:
      dic[state] = p
  else:
    if current_state in dic.keys():
      dic[current_state] += p
    else:
      dic[current_state] = p

def get_transition_probs(dic, main_state, left_state, right_state, current_state):
    add_to_dic(dic, main_state, current_state, 0.8)
    add_to_dic(dic, left_state, current_state, 0.05)
    add_to_dic(dic, right_state, current_state, 0.05)
    if current_state in dic.keys():
        dic[current_state] += 0.10
    else:
      dic[current_state] = 0.10

def get_next_state_probs(current_state, action):
  dic = {}
  if action == 'AU':
    main_state = go_up(current_state)
    left_state = go_left(current_state)
    right_state = go_right(current_state)
    get_transition_probs(dic, main_state, left_state, right_state, current_state)

  if action == 'AD':
    main_state = go_down(current_state)
    left_state = go_right(current_state)
    right_state = go_left(current_state)
    get_transition_probs(dic, main_state, left_state, right_state, current_state)

  if action == 'AL':
    main_state = go_left(current_state)
    left_state = go_down(current_state)
    right_state = go_up(current_state)
    get_transition_probs(dic, main_state, left_state, right_state, current_state)

  if action == 'AR':
    main_state = go_right(current_state)
    left_state = go_up(current_state)
    right_state = go_down(current_state)
    get_transition_probs(dic, main_state, left_state, right_state, current_state)

  return dic

def get_max_action(current_state, prev_values, gamma, gold, terminal_states):
  max_action = ''
  max_action_value = float('-inf')
  if current_state in terminal_states:
    return 'G', 0
  for action in ['AU', 'AD', 'AL', 'AR']:
    a = get_next_state_probs(current_state, action)
    total = 0
    for key, value in a.items():
      total += (value*get_reward(key, gold)) + (value*gamma*prev_values[key])
    if total > max_action_value:
      max_action = action
      max_action_value = total
  return max_action, max_action_value

def arrow_representation(state_actions):
  dic = {'AU':'\u2191', 'AD':'\u2193', 'AL':'\u2190', 'AR':'\u2192', 'G':'G'}
  for i in range(len(state_actions)):
    for j in range(len(state_actions)):
      state_actions[i][j] = dic.get(state_actions[i][j], state_actions[i][j])
  return state_actions

def rounded_representation(state_values):
  for i in range(len(state_values)):
    for j in range(len(state_values)):
      state_values[i][j] = round(state_values[i][j], 4)
  return state_values

def get_q_state_index(state):
  return state[0] * 5 + state[1]

def get_q_action_index(action):
  if action == 'AU':
    return 0
  elif action == 'AD':
    return 1
  elif action == 'AL':
    return 2
  else:
    return 3

def update_model(current_state, current_action, next_state):
  if next_state in model[(current_state, current_action)].keys():
    model[(current_state, current_action)][next_state] += 1
  else:
    model[(current_state, current_action)][next_state] = 1

#############################################################################################
#################################### VI #####################################################
#############################################################################################

state_values = np.zeros((5, 5))
state_actions = np.random.choice(['AU', 'AD', 'AL', 'AR'], size=(5, 5), p=[0.25, 0.25, 0.25, 0.25])
state_actions[(2, 2)] = ' '
state_actions[(3, 2)] = ' '
gamma = 0.9
gold = 0
terminal_states = [(4, 4)]


counter = 0
while True:
  delta = 0
  counter += 1
  next_state_values = np.zeros((5, 5))
  for i in range(len(state_values)):
    for j in range(len(state_values)):
      if is_valid_state((i, j)):
        temp = state_values[i][j]
        state_actions[i][j], next_state_values[i][j] = get_max_action((i, j), state_values, gamma, gold, terminal_states)
        delta = max(delta, abs(temp - next_state_values[i][j]))
  if delta < 0.0001:
    break
  state_values = next_state_values.copy()
vi_values = rounded_representation(state_values)
print(rounded_representation(state_values))
print(arrow_representation(state_actions))
print("Iterations ->"+str(counter))

def get_action_from_index(action_index):
  if action_index == 0:
    return 'AU'
  elif action_index == 1:
    return 'AD'
  elif action_index == 2:
    return 'AL'
  else:
    return 'AR'

def get_epsilon_greedy_action(q_values, state, epsilon):
  if all(x == q_values[get_q_state_index(state)][0] for x in q_values[get_q_state_index(state)]):
    return np.random.choice(['AU', 'AD', 'AL', 'AR'], p=[0.25, 0.25, 0.25, 0.25])
  max_ind = np.where(q_values[get_q_state_index(state)] == max(q_values[get_q_state_index(state)]))[0][0]
  choices = list(range(4))
  choices.remove(max_ind)
  choices.insert(0, max_ind)
  p = [1 - epsilon + (epsilon/4), epsilon/4, epsilon/4, epsilon/4]
  action_index = np.random.choice(choices, p=p)
  return get_action_from_index(action_index)

def update_model(current_state, current_action, next_state):
  if next_state in model[(current_state, current_action)].keys():
    model[(current_state, current_action)][next_state] += 1
  else:
    model[(current_state, current_action)][next_state] = 1

def get_next_state(current_state, current_action):
  next_state_probs = get_next_state_probs(current_state, current_action)
  next_state_index = np.random.choice([i for i in range(len(next_state_probs.values()))], p=list(next_state_probs.values()))
  next_state = list(next_state_probs.keys())[next_state_index]
  return next_state

def update_q_value(state, action, gold):
  a = model[(state, action)]
  keys = list(a.keys())
  vals = [val/sum(a.values()) for val in a.values()]

  s = 0
  for i in range(len(keys)):
    max_q_value = max(q_values[get_q_state_index(keys[i])])
    s += vals[i] * (get_reward(keys[i], gold) + (gamma * max_q_value))
  return s

def get_policy(q_values):
  lst = []
  empty_2d_array = np.random.choice(['AU', 'AD', 'AL', 'AR'], size=(5, 5), p=[0.25, 0.25, 0.25, 0.25])
  for i in range(len(q_values)):
    lst.append(np.where(q_values[i] == max(q_values[i]))[0][0])
  lst.reverse()

  for i in range(5):
    for j in range(5):
      if not is_valid_state((i, j)):
        empty_2d_array[(i, j)] = ' '
        lst.pop()
        continue
      if (i, j) in terminal_states:
        empty_2d_array[(i, j)] = 'G'
        lst.pop()
        continue
      empty_2d_array[(i, j)] = get_action_from_index(lst.pop())

  return arrow_representation(empty_2d_array)

def get_SARSA_value_fnc(q_values):
  vals = []
  for k in range(len(q_values)):
    m = max(q_values[k])
    s = 0
    for i in range(len(q_values[k])):
      if q_values[k][i] == m:
        s += (1 - epsilon + (epsilon/4)) * q_values[k][i]
      else:
        s += (epsilon/4 * q_values[k][i])
    vals.append(s)
  vals.reverse()
  val_fnc = np.zeros((5, 5))
  for i in range(5):
    for j in range(5):
      val_fnc[i, j] = vals.pop()
  return val_fnc

def get_mean_square_error(value_fns, optimal_value_fn):
  errors = []
  for fn in value_fns:
    errors.append(np.mean((fn - optimal_value_fn) ** 2))
  return errors

epsilon = 0.5
gold = 0
gamma = 0.9
every_episode_q_values_outer = []
for _ in range(20):
  q_values = np.zeros((25, 4))
  model = {((i, j), k): {} for i in range(5) for j in range(5) for k in ['AU', 'AD', 'AL', 'AR'] if is_valid_state((i, j)) and (i, j) not in terminal_states}
  pri_queue = {}
  terminal_states = [(4, 4)]
  every_episode_q_values_inner = []
  action_counter = 0
  for _ in range(4000):
    every_episode_q_values_inner.append(q_values.copy())
    current_state = (2, 2)
    while (not is_valid_state(current_state) or current_state in terminal_states):
      current_state = (np.random.randint(0, 5), np.random.randint(0, 5))
    current_action = get_epsilon_greedy_action(q_values, current_state, epsilon)
    next_state = get_next_state(current_state, current_action)
    update_model(current_state, current_action, next_state)
    reward = get_reward(next_state, gold)
    max_q_value = max(q_values[get_q_state_index(next_state)])
    P = abs(reward + (gamma * max_q_value) - q_values[get_q_state_index(current_state), get_q_action_index(current_action)])
    if P > 0.001:
      pri_queue[(current_state, current_action)] = P
      pri_queue = dict(sorted(pri_queue.items(), key=lambda item: item[1]))
    for i in range(5):
      if len(pri_queue) > 0:
        t_state, t_action = pri_queue.popitem()[0]
        q_values[get_q_state_index(t_state), get_q_action_index(t_action)] = update_q_value(t_state, t_action, gold)
        previous_pairs = [key for key, value in model.items() if t_state in value]
        for pair in previous_pairs:
          max_inner_q_value = max(q_values[get_q_state_index(t_state)])
          inner_P = abs(get_reward(t_state, gold) + (gamma * max_inner_q_value) - q_values[get_q_state_index(pair[0]), get_q_action_index(pair[1])])
          if inner_P > 0.001:
            if pair not in pri_queue.keys():
              pri_queue[pair] = inner_P
              pri_queue = dict(sorted(pri_queue.items(), key=lambda item: item[1]))
            else:
              if pri_queue[pair] < inner_P:
                pri_queue[pair] = inner_P
                pri_queue = dict(sorted(pri_queue.items(), key=lambda item: item[1]))
  every_episode_q_values_outer.append(every_episode_q_values_inner.copy())

every_episode_q_values = []
for i in range(4000):
  s = []
  for j in range(20):
    s.append(every_episode_q_values_outer[j][i].copy())
  every_episode_q_values.append(np.mean(s, axis=0))

every_episode_value_fn = []
for i in range(len(every_episode_q_values)):
  every_episode_value_fn.append(get_SARSA_value_fnc(every_episode_q_values[i]))

mse = get_mean_square_error(every_episode_value_fn, vi_values)

x = range(len(mse))
y = mse
plt.plot(x, y)
plt.ylabel('Average MSE per episode')
plt.xlabel('Episodes')
plt.title('20 Iterations of Prioritized Sweeping (Episodes vs Avg MSE Graph)')
plt.show()

print('Final Policy after 4000 iterations ->')
print(get_policy(every_episode_q_values[3999]))